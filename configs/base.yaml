# NAT Base Configuration â€” Qwen3-4B on A100-80GB
# Two-phase training: Phase 1 (episodic meta-learning), Phase 2 (consolidation)

# Model
base_model_name: "Qwen/Qwen3-4B"
rank: 64
d_hidden: 512

# Adaptation
adapt_every_n: 256
lr_clamp: 0.05
fast_weight_max_norm: 8.0

# Consolidation
beta: 0.999
session_reset_alpha: 0.5

# Training - Phase 1 (episodic multi-domain meta-learning)
lr_phase1: 2e-4
num_episodes_p1: 50000
batch_size: 4
seq_len: 2048
truncated_bptt: 4
grad_clip: 1.0
weight_decay: 0.01
improvement_weight: 0.1
num_problems_per_episode: 8
adapt_problems_p1: 5

# Training - Phase 2 (consolidation across domains)
lr_phase2: 1e-4
num_runs_p2: 500
sessions_per_domain_p2: 20
forgetting_test_sessions_p2: 5
p2_truncate_sessions: 4

# Device
device: "auto"  # auto-detect: cuda > mps > cpu
base_dtype: "bfloat16"

# Performance (A100-80GB)
# gradient_checkpointing is incompatible with stateful hooks (fast_A/prev_h
# mutation during adapt causes checkpoint recompute tensor-count mismatch).
# A100-80GB has ample VRAM without it (~20 GB peak with batch_size 4).
gradient_checkpointing: false
compile_model: false
tf32_matmul: true

# Logging
wandb_project: "nat"
wandb_entity: null
log_every: 50

# Saving
save_dir: "checkpoints"
save_every: 1000
