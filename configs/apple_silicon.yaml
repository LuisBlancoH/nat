# NAT Apple Silicon Configuration
# Optimised for M1/M2/M3/M4 Macs (16-36 GB unified memory)
#
# Key constraints:
#   - Unified memory shared between CPU and GPU — keep batch small
#   - MPS backend doesn't support torch.compile
#   - bfloat16 is supported on MPS (PyTorch 2.1+)
#   - Gradient checkpointing recommended to fit in 16 GB
#
# Expected timeline (M1 16 GB):
#   Phase 1: ~3-5 days
#   Phase 2: ~3-5 days
#   Phase 3: ~1-2 weeks
#   Total:   ~3-4 weeks

# Model
base_model_name: "Qwen/Qwen2.5-1.5B"
rank: 32
d_hidden: 256

# Adaptation — less frequent to reduce peak memory
adapt_every_n: 64
lr_clamp: 0.1
fast_weight_max_norm: 10.0

# Consolidation
beta: 0.999
session_reset_alpha: 0.5

# Training - Phase 1
lr_phase1: 3e-4
num_episodes_p1: 50000
batch_size: 1              # memory-limited
seq_len: 1024              # shorter sequences for MPS
truncated_bptt: 8          # shorter BPTT window saves memory
grad_clip: 1.0
weight_decay: 0.01

# Training - Phase 2
lr_phase2: 3e-4
num_episodes_p2: 30000
improvement_weight: 0.1
num_problems_per_episode: 4   # fewer problems per episode to fit in memory

# Training - Phase 3
lr_phase3: 1e-4
num_runs_p3: 100
sessions_per_domain_p3: 20
forgetting_test_sessions_p3: 5
p3_truncate_sessions: 2       # more aggressive truncation for memory

# Device
device: "mps"
base_dtype: "bfloat16"

# Performance
gradient_checkpointing: true   # saves ~30-40% memory, ~15% slower
compile_model: false           # MPS backend does not support torch.compile
num_workers: 0                 # MPS works best with main-process loading
pin_memory: false              # not needed with unified memory
empty_cache_every: 100         # call torch.mps.empty_cache() periodically

# Logging
wandb_project: "nat"
wandb_entity: null
log_every: 50

# Saving
save_dir: "checkpoints"
save_every: 1000
